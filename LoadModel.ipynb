{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P0v-2rm7yEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5331815-cfba-4c89-8e53-328bde551835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.31.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxm0YCb-8CQE",
        "outputId": "3f1fa319-24ac-4864-d80e-d3268c066018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "id": "0op6o_g-72EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lFHqphmUtNAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/STACKOverflow/Llama\")\n",
        "tokenizer= AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/STACKOverflow/Llama\")"
      ],
      "metadata": {
        "id": "02JZJ2nx8U_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "TBo8_Ri8tOqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My system is crashing\"\n",
        "input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output = model.generate(input_ids, max_length=250, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, num_beams=5)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "pattern = r\"\\[/INST\\](.*?)(?:\\[/INST\\]|$)\"\n",
        "match = re.search(pattern, output_text)\n",
        "if match:\n",
        "    answer = match.group(1).strip()\n",
        "else:\n",
        "    answer = output_text\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "xjtF59bQ8Yxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}